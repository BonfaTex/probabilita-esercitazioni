%!TEX root = ../main.tex

%

Vediamo le principali definizioni e i principali teoremi che serviranno per affrontare gi esercizi di questo capitolo:
\begin{definition}$\\$
Sia $\PP$ una probabilità su $(\RR,\Bc)$, la funzione caratteristica (o \emph{trasformata di Fourier}) di $\PP$ è la funzione
\begin{gather*}
\begin{aligned}
\widehat{\PP}=\varphi: \RR^n &\to \CC\\
u&\mapsto\varphi (u):= \int_{\RR^n}e^{i\langle u|x\rangle}\ \PP(\text{d}x)
\end{aligned}
\end{gather*}
\end{definition}
Dalla \emph{formula di Eulero} sappiamo che $e^{i \vartheta}=\cos(\vartheta) + i \sin (\vartheta)$ e $|e^{i \vartheta}|=1$ per ogni $\vartheta\in\RR$. Ne discende che
\begin{gather*}
\begin{aligned}
&e^{i\langle u|x\rangle}=\cos(\langle u|x\rangle) + i \sin (\langle u|x\rangle) \\
&|e^{i \langle u|x\rangle}|=1
\end{aligned}
\end{gather*}
per ogni $u,x\in\RR^n$.
\begin{definition}$\\$
\label{carmom}
Sia $X:\SDP\to(\RR^n,\Bc^n)$ vettore aleatorio (continuo, discreto, altro) di legge $P^X$. La funzione caratteristica di $X$ è $\widehat P^X =\varphi_X:\RR^n\to \CC$ definita come
\[
\varphi_X(u)=\int_{\RR^n}e^{i \langle u|X\rangle}\ P^X(\text{d}x)=\EE\left[e^{i \langle u|X\rangle}\right]=\int_\Omega e^{i \langle u|X\rangle}\dP
\]
\end{definition}
\emph{Esempio}. Sia $X\sim\delta_t$ con $t\in\RR$. Allora
\[
\varphi_X(u)=\EE\left[e^{iuX}\right]=e^{iut}
\]
\emph{Esempio}. Sia $X\sim\PPP(\lambda)$ con $\lambda\in\RR_+$. Allora
\begin{gather*}
\begin{aligned}
\varphi_X(u) &=\EE[e^{i u X}] =\sum_{k=0}^{{+\infty}}e^{i u k}\ e^{-\lambda}\ \frac{\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^{{+\infty}}\frac{(\lambda e^{i u})^k}{k!}=\\
&=\left\{ \sum_{k=0}^{+\infty}\frac{x^k}{k!}=e^x\ \ \ \forall x\in\RR  \right\}=e^{-\lambda}\ e^{\lambda e^{iu}}=e^{ \lambda (e^{i u}-1)}
\end{aligned}
\end{gather*}
\begin{theorem}[La funzione caratteristica di una probabilità caratterizza la probabilità]$\\$
\label{La funzione caratteristica di una probabilità caratterizza la probabilità}
La funzione caratteristica $\widehat{\PP}$ di una probabilità $\PP$ su $(\RR^n, \Bc^n)$ caratterizza $\PP$, ovvero
\[
\widehat{\PP}(u)=\widehat{\QQ}(u) \ \forall u \in \RR^n\ \Longleftrightarrow\ \PP(B)=\QQ(B) \ \forall B \in \Bc^n
\]
\end{theorem}
\begin{theorem}$\\$
\label{teo dei momenti}
Sia $X=(X_1,\dots,X_n):\SDP\to(\RR^n,\Bc^n)$ vettore aleatorio tale che per qualche $m\in\NN$ si abbia \\ $X_k \in L^m (\PP)$ $\forall k=1, \dots,  n$. Allora $\varphi_X\in \mathcal{C}^m(\RR^n,\CC)$ e
\[
\frac{\partial^m}{\partial u_{k_1} \cdots \partial u_{k_m}}\ \varphi(u)=i^m\ \EE\left[X_{k_1} \cdots X_{k_m}\ e^{i \langle u|x\rangle}\right] \qquad \forall u \in \RR^n
\]
\end{theorem}
\emph{Esempio}. Data $X\sim \NNN(0,1)$ si vuole calcolare $\varphi_X(u)$. \\
Prima di tutto osserviamo che $X\in L^m$ $\forall m\geq 1$. Sostituendo nella definizione di funzione caratteristica la densità della normale standard si ottiene
  \begin{equation*}
    \varphi_X (u) = \EE[e^{i u X}]=\int_{\RR}e^{iut}\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}\dt
  \end{equation*}
Dalla formula di Eulero abbiamo $e^{iut}=\cos(ut)+i\sin(ut)$, quindi
\[
 \varphi_X (u) =\int_{\RR} \cos(ut)\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}\dt + i\underbrace{\int_{\RR}\sin(ut)\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}\dt}_{0\text{ perché dispari}}=\frac{1}{\sqrt{2\pi}}\int_{\RR} \cos(ut)\ e^{-\frac{t^2}{2}}\dt
\]
Per risolvere quest'integrale si usa un trucco: applicando il teorema (\ref{teo dei momenti}) si ottiene la derivata $\varphi_X' (u)$ in funzione di $\varphi_X (u)$, cioè
\begin{gather*}
\begin{aligned}
 \varphi_X' (u) & = i \ \EE\left[X e^{i u X}\right] = i \int_\RR t e^{iut}\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}\dt= \\
&= i \left(\underbrace{ \int_\RR t \cos(ut)\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}}_{0\text{ perché dispari}}\dt
    + i \int_\RR t \sin(ut)\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}\dt \right)=\\
&= -\int_\RR t \sin(ut)\ \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}\dt=\\
&\overset{\underset{pp}{}}{=}-\frac{1}{\sqrt{2 \pi}}\left\{ \left[ -e^{-\frac{t^2}{2}} \sin (ut) \right]_{-\infty}^{+\infty} +\int_{-\infty}^{+\infty}ue^{\frac{t^2}2} \cos (ut)\dt \right\}=\\
&=-u \varphi_X (u)
\end{aligned}
\end{gather*}
L'integrale si è così trasformato in un \emph{problema di Cauchy} ben posto:
  $$\begin{cases}
    \varphi_X'(u)=-u\varphi_X(u) \\ \varphi_X(0)=1
  \end{cases} \quad \Longrightarrow \quad \boxed{\varphi_X(u)=e^{-\frac{u^2}{2}}}$$
Siano $X:\SDP\to(\RR^n,\Bc^n)$ un vettore aleatorio e $Y:\SDP\to(\RR^m,\Bc^m)$ con $A\in\RR^{m\times m},b\in\RR^m$ la trasformazione affine $Y=AX+b$.
\begin{theorem}$\\$
\label{t_affine_t}
La funzione caratteristica del vettore $Y$ dato dalla trasformazione affine $Y=AX+b$ è
\[
\varphi_Y(u)=e^{i\langle u|b\rangle} \ \varphi_X\left(A^T  u\right) \ \ \ \forall u \in \RR^m
\]
\end{theorem}
\emph{Esempio}. Calcolare la funzione caratteristica di una normale generica.\\
Sia $X\sim N(\mu,\sigma^2)$. Allora basta vedere $X=\mu+\sigma Z$, con $Z=\dfrac{X-\mu}{\sigma}\sim \NNN(0,1)$. Allora
$$\varphi_X (u)=e^{i u \mu}\ \varphi_Z(\sigma u)=  e^{i u \mu}\ e^{-\sigma^2u^2/2} =e^{i u \mu-\sigma^2u^2/2}$$
\emph{Esempio}. Calcolare la funzione caratteristica della somma di variabili aleatorie.\\
Siano $X=(X_1,  \dots,  X_n) \sim \varphi_X $ e $ Y=\displaystyle\sum_{k=1}^{n}X_k$. Allora vedendo $Y$ come trasformazione affine
\[
Y=\begin{pmatrix}
1 &\cdots  &1  \\
\end{pmatrix}
\begin{pmatrix}
X_1 \\ \vdots
 \\ X_n
\end{pmatrix} +0
\]
possiamo calcolare
\[
\varphi_Y(u) = \varphi_X \left(
      \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} u
    \right) = \varphi_X(u,  \dots,  u)
\]
\begin{oss}$\\$
Quest'ultimo esempio semplifica tantissimo il calcolo delle funzioni caratteristiche marginali di un vettore. Infatti dato $X=(X_1,  \dots,  X_n) \sim \varphi_X $ per calcolare la funzione caratteristica $\varphi_{X_k}$ vediamo $X_k=(0,\dots,0,1,0,\dots,0)X=vX$ e dunque
\[
\varphi_{X_k}(u) = \varphi_X\left(v u\right)=\varphi_X(v)=\varphi_X(0,\dots,0,u,0,  \dots,  0)
\]
\end{oss}
\begin{theorem}[Fattorizzazione della funzione caratteristica per famiglie di VA]
\label{Fattorizzazione della funzione caratteristica per famiglie di VA}
$\\$
Sia $X = (X_1,  \dots, X_n):\SDP\to(\RR^n,\Bc^n)$ vettore aleatorio. Allora
      $$\big\{X_k\big\}_{k=1}^{n} \text{ famiglia di VAR} \indep\ \Longleftrightarrow\ \varphi_X(u_1, \dots, u_n) = \prod_{k=1}^{n} \varphi_{X_k} (u_k) \quad \forall u \in \RR^n$$
\end{theorem}
\begin{corollario}[Legge della somma di variabili indipendenti]$\\$
\label{Legge della somma di variabili indipendenti}
Siano $X_1, \dots , X_n$ VAR indipendenti. Allora
  $$Y = \sum_{k=1}^{n} X_k\ \Longrightarrow\ \varphi_Y(u) = \varphi_{\sum X_k} (u) = \prod_{k=1}^{n} \varphi_{X_k}(u) \quad \forall u \in \RR$$
\end{corollario}

%

Prima procedere con le soluzioni degli esercizi, ci teniamo a fare ulteriori precisazioni in modo da rendere più scorrevoli gli esercizi a venire. 

Abbiamo già visto che: 
\begin{itemize}
\item $X\sim\delta_t\SE\varphi_X(u)=e^{iut}$
\item $X\sim\PPP(\lambda)\SE\varphi_X(u)=e^{ \lambda (e^{i u}-1)}$
\item $Z\sim\NNN(0,1)\SE\varphi_X(u)=e^{-u^2/2}$
\item $X\sim\NNN(\mu,\sigma^2)\SE\varphi_X(u)=e^{iu\mu-\sigma^2u^2/2}$
\end{itemize}
tuttavia utilizzando il teorema (\ref{La funzione caratteristica di una probabilità caratterizza la probabilità}) possiamo dire che l'implicazione vale anche in senso opposto, ovvero (parafrasando tale teorema)
\begin{theorem}
\label{caratterizzazione della f caratteristica}
Date $X,Y:\Omega\to\RR^n$ si ha $P^X=P^Y\SESOLOSE\varphi_X=\varphi_Y$.
\end{theorem}
Ecco quindi i primi quattro risultati notevoli di questo capitolo
\[
X\sim\delta_t,\ t\in\RR\SESOLOSE\varphi_X(u)=e^{iut} \qquad X\sim\PPP(\lambda),\ \lambda>0\SESOLOSE\varphi_X(u)=e^{ \lambda (e^{i u}-1)}
\]
\[
Z\sim\NNN(0,1)\SESOLOSE\varphi_X(u)=e^{-u^2/2} \qquad X\sim\NNN(\mu,\sigma^2),\ \mu\in\RR,\ \sigma^2>0\SESOLOSE\varphi_X(u)=e^{iu\mu-\sigma^2u^2/2}
\]
I richiami al teorema (\ref{caratterizzazione della f caratteristica}) verranno omessi, e verrà dimostrato quindi solo il $\SE$.

Verrà anche omesso il riferimento alla definizione (\ref{carmom}) quando scriveremo $\varphi_X(u)=\EE\left[e^{i \langle u|X\rangle}\right]$.

Inoltre verrà fatto uso delle seguenti formule
\begin{enumerate}
\item [(EU)] Formula di Eulero
\[
e^{i \vartheta}=\cos(\vartheta) + i \sin (\vartheta)
\]
\item [(NT)] Binomio di Newton
\[
(a+b)^n=\sum_{k=0}^n\binom{n}{k}a^{n-k}\ b^k
\]
\item [(TY)] Sviluppi in serie di Taylor - Mc Laurin
\begin{gather*}
\begin{aligned}
\cos(x)&=\sum_{n=0}^\infty\frac{(-1)^n}{(2n)!}\ x^{2n}\qquad\forall x\in\RR \\
&=1-\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots +o\left(x^{2n}\right)\\
\sin(x)&=\sum_{n=0}^\infty\frac{(-1)^n}{(2n+1)!}\ x^{2n+1}\qquad\forall x\in\RR\\
&=x-\frac{x^3}{3!}+\frac{x^5}{5!}+\cdots+o\left(x^{2n+1}\right)
\end{aligned}
\end{gather*}
\end{enumerate}

\newpage

%

\ParteEsercizi

\Esercizio{} %1
Sia $\varphi_X$ la funzione caratteristica di una variabile aleatoria reale $X$, quindi $X\sim\varphi_X$.
\begin{enumerate}
\item [(a)] Si mostri che $Y=-X\sim\overline{\varphi}_X$ e che $\overline{\varphi}_X(u)=\varphi_X(-u)$ ($\overline{\ \cdot\ }$ è il complesso coniugato).
\item [(b)] Si mostri che, se le variabili aleatorie $X$ e $W$ sono i.i.d. con funzione caratteristica $\varphi$, allora $X-W\sim|\varphi|^2$.
\end{enumerate}

\Esercizio{} %2
Siano $ n,m \in \NN$, $p\in [0, 1]$ e $\varphi_X$ la funzione caratteristica di una variabile aleatoria $X$.
\begin{enumerate}
\item [(a)] Si mostri che $X\sim B(p)\SESOLOSE\varphi_X(u)=pe^{iu}+1-p$.
\item [(b)] Si mostri che $X\sim B(n,p)\SESOLOSE\varphi_X(u)=\left(pe^{iu}+1-p  \right)^n$.
\item [(c)] Si mostri che $X_1,\dots,X_n\iid B(p)\SE X_1+\cdots+X_n\sim B(n,p)$.
\item [(d)] Si mostri che $X\sim B(n,p)\indep Y\sim B(m,p)\SE X+Y\sim B(n+m,p)$.
\end{enumerate}

\Esercizio{} %3
Data una variabile aleatoria reale $X$, si mostri che
\[
X\sim \GGG(p),\ p\in(0,1)\SESOLOSE \varphi_X(u)=\frac{pe^{iu}}{1-e^{iu}(1-p)}
\]

\Esercizio{} %4
Sia $X$ una variabile aleatoria reale.
\begin{enumerate}
\item [(a)] Si mostri che $X\sim \UUU([-a,a])\SESOLOSE\varphi_X(u)=\dfrac{\sin(au)}{au}$.
\item [(b)] Si mostri che $X\sim  \UUU([a,b])\SESOLOSE\varphi_X(u)=e^{iu\frac{a+b}{2}}\ \dfrac{\sin\left(\frac{b-a}{2}u\right)}{\frac{b-a}{2}u}$.
\item [(c)] Si mostri che $X\sim\EEE(\lambda)\SESOLOSE\varphi_X(u)=\dfrac{\lambda}{\lambda-iu}$.
\item [(d)] Si mostri che $X$ tale che $-X\sim\EEE(\lambda)\SESOLOSE\varphi_X(u)=\dfrac{\lambda}{\lambda+iu}$.
\end{enumerate}
Si calcolino valore atteso e varianza per queste variabili aleatorie a partire dalle loro funzioni caratteristiche.

\Esercizio{} %5
Sia $(X,Y)$ un vettore aleatorio uniformemente distribuito sul rettangolo $R=\{0\leq x\leq 2,\ 0\leq y\leq 1\}$.
\begin{itemize}
\item [(a)] Si scrivano le densità continue di $(X,Y)$, di $X$ e di $Y$. Le variabili aleatorie $X$ e $Y$ sono indipendenti?
\item [(b)] Si calcolino valore atteso e matrice varianza di $(X,Y)$.
\item [(c)] Si ricavi la funzione caratteristica di $(X,Y)$ in termini delle funzioni caratteristiche di $X$ e $Y$.
\item [(d)] Siano $Z=2X-Y,\ W=Y-2,\ J=-X$. Il vettore aleatorio $(Z,W,J)$ ammette densità continua?
\item [(e)] Si calcolino valore atteso e matrice varianza di $(Z,W,J)$.
\item [(f)] Si ricavi la funzione caratteristica di $(Z,W,J)$ in termini delle funzioni caratteristiche di $X$ e $Y$.
\item [(g)] Si ricavino le funzioni caratteristiche di $(Z,W)$ e $(W,J)$ in termini della funzione caratteristica di $(Z,W,J)$.
\end{itemize}

\Esercizio{} %6
Siano $X_1,\dots,X_n$ variabili aleatorie indipendenti, con $X_k\sim\PPP(\lambda_k),\ k=1,\dots,n$.
\begin{itemize}
\item [(a)] Mostrare che $X_1+\cdots+X_n\sim\PPP(\lambda_1+\cdots+\lambda_n)$.
\item [(b)] Si supponga ora che $X_1,X_2,X_3\iid\PPP(\lambda)$. Calcolare $\PP(X_1+X_2+X_3\geq 3\,|\,X_1\geq 1)$. 
\end{itemize}

\Esercizio{} %7
Date $X_1,\dots,X_n$ variabili aleatorie reali i.i.d. si mostri che
\[
\overline{X}_n=\displaystyle\frac{1}{n}\sum_{k=1}^n X_k,\ n\in\NN\SESOLOSE \varphi_{\overline{X}_n}(u)=\left(\varphi_{X_1}\left(\frac{u}{n}\right)\right)^n
\]

\Esercizio{} %8
Per ogni $\alpha>0$ e $\lambda>0$, la funzione caratteristica di una variabile aleatoria $X\sim\Gamma(\alpha,\lambda)$ è data da
\[
\varphi_X(u)=\left( \frac{\lambda}{\lambda-iu} \right)^\alpha
\]
\begin{enumerate}
\item [(a)] Si mostri che $X\sim\Gamma(\alpha,\lambda)\indep Y\sim\Gamma(\beta,\lambda)\SE X+Y\sim\Gamma(\alpha+\beta,\lambda)$.
\item [(b)] Si mostri che $X_1,\dots,X_n\iid\EEE(\lambda)\SE X_1+\cdots+X_n\sim\Gamma(n,\lambda)$.
\item [(c)] Si mostri che $Z\sim\NNN(0,1)\SE Z^2\sim\Gamma\left(\frac{1}{2},\frac{1}{2}\right)=\chi^2(1)$.
\item [(d)] Si mostri che $Q=Z_1^2+\cdots+Z_n^2$, con $Z_1,\dots,Z_n\iid\NNN(0,1)\SE Q\sim\Gamma\left(\frac{n}{2},\frac{1}{2}\right)=\chi^2(n)$.
\end{enumerate}

\Esercizio{} %9
Siano $X\sim\Gamma(\alpha,\lambda)$ e $Y\sim\Gamma(\beta,\lambda)$ indipendenti, con $\alpha>0,\ \beta>0$ e $\lambda>0$.
\begin{enumerate}
\item [(a)] Si considerino le variabili aleatorie
\[
T=X+Y\qquad U=\frac{X}{X+Y}
\]
Si mostri che $(T,U)$ ammette densità continua e la si calcoli.
\item [(b)] Si provi che $T\indep U$.
\item [(c)] Si provi che $T\sim\Gamma(\alpha+\beta,\lambda)$ e che $U\sim\text{Beta}(\alpha,\beta)$, ossia $U$ ha distribuzione Beta di parametri $\alpha$ e $\beta$, la cui densità continua è data da
\[
f_U(u)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\ \Gamma(\beta)}\ u^{\alpha-1}\ (1-u)^{\beta-1}\ \Ind_{(0,1)}(u).
\]
\item [(d$^\ast$)] Sia $(X_n)_{n\in\NN}$ una successione di variabili aleatorie, con $X_1,X_2,\dots\iid\EEE(\lambda)$. Posto
\[
S_n=X_1+\dots+X_n
\]
si provi che per ogni coppia di interi $(k,n)$ con $1\leq k\leq n$, la variabile aleatoria $S_{k/n}=\dfrac{S_k}{S_n}$ è indipendente da $S_n$. Sfruttando quest'ultimo risultato si calcoli il valore atteso di $S_{k/n}$.
\end{enumerate}

\Esercizio{} %10
Siano $X$ e $Y$ variabili aleatorie congiuntamente gaussiane
\begin{enumerate}
\item [(a$^*$)] Si descriva l'immagine $S$ del vettore aleatorio $(X,Y)$ al variare di $\mu_X,\mu_Y,\sigma_X,\sigma_Y, Cov(X,Y)$.
\item [(b)] Si mostri che $(X,Y)$ ammette densità continua $\fXY\SESOLOSE\sigma_X>0,\sigma_Y>0,|\rho_{X,Y}|<1$.
\item [(c)] Si mostri che, quando $(X,Y)$ ammette densità continua, si ha
\begin{gather*}
\begin{aligned}
\fXYxy=&\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho_{X,Y}^2}}\\ &\times\exp\left\{-\frac{1}{2(1-\rho_{X,Y}^2)}\left[\frac{(x-\mu_X)^2}{\sigma_X^2}-2\rho_{X,Y}\frac{(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}+\frac{(y-\mu_Y)^2}{\sigma_Y^2}  \right]\right\}.
\end{aligned}
\end{gather*}
\item [(d)] Si discuta la dipendenza del grafico di $\fXY$ dai parametri $\sigma_X,\sigma_Y,\rho_{X,Y}$.
\end{enumerate}

\Esercizio{} %11
Sia $(X,Y)$ un vettore gaussiano. Le densità di $X$ e di $Y$ sono
\[
f_X(x)=\frac{\sqrt{3}}{\sqrt{\pi}}e^{-3x^2}\qquad\qquad f_Y(y)=\frac{3\sqrt{3}}{2\sqrt{\pi}}e^{-\frac{27}{4}y^2}\qquad\qquad x,y\in\RR
\]
Il coefficiente di correlazione è $\rho_{X,Y}=1/2$.
\begin{enumerate}
\item [(a)] Riconoscere le leggi di $X$ e di $Y$.
\item [(b)] Scrivere media, matrice varianza e densità continua di $(X,Y)$.
\item [(c)] Calcolare la legge di $Z=X+3Y$.
\item [(d)] Calcolare $\PP(X>Y)$.
\end{enumerate}

\Esercizio{} %12
Date $X$ e $Y$ indipendenti di legge $\NNN(0,1)$, si trovi la legge congiunta di $U=X+Y$ e $V=X-Y$. $U\indep V$?

\Esercizio{} %13
Siano $X$ e $Y$ due variabili aleatorie indipendenti, tali che $X\sim\NNN(0,1)$ ed $Y\sim\NNN(0,4)$.
\begin{enumerate}
\item Determinare la legge di $Z=X+Y$.
\item Determinare la matrice varianza del vettore $(X,Z)$.
\item Determinare la legge congiunta di $X$ e $Z$. Ammette densità continua? Se sì, quale?
\item Determinare la funzione caratteristica di $(X,Z)$.
\item Sia $W\sim\NNN(1,2)$, indipendente dal vettore $(X,Z)$. Qual è la legge di $(X,Z,W)$?
\end{enumerate}

\Esercizio{} %14
Dati $X\sim\NNN(0,1)$ e $a>0$, si consideri $\begin{cases}X,&|X|<a,\\-X,&|X|\geq a.  \end{cases}$
\begin{enumerate}
\item [(a)] Si trovi lalegge di $Y$.
\item [(b)] Si stabilisca se i vettori $(X,X)$ e $(X,Y)$ sono gaussiani.
\item [(c)] Si calcoli $\PP(X>Y)$.
\item [(d)] Si trovi il coefficiente di correlazione$\rho_{X,Y}$.
\end{enumerate}

\Esercizio{} %15
Sia $X=(X_1,X_2,X_3$ un vettore gaussiano $\NNN(\mu,C)$, dove
\[
\mu=\begin{pmatrix}
0 \\0
 \\1
\end{pmatrix},\qquad\qquad C=\begin{pmatrix}
1 & 1 & 0 \\
1 & 2 & 0 \\
0 &  0& 1 \\
\end{pmatrix}.
\]
\begin{enumerate}
\item [(a)] Il vettore $(X_1,X_2)$ e la variabile $X_3$ sono indipendenti?
\item [(b)] La variabile $X_1$ e il vettore $(X_2,X_3)$ sono indipendenti?
\item [(c)] Determinare, al variare di $\lambda\in\RR$, la legge del vettore $(U,V)$ definito come segue:
\[
\begin{pmatrix}
U \\V

\end{pmatrix}=\begin{pmatrix}
X_1 \\X_2+\lambda X_3
\end{pmatrix}.
\]
\item [(d)] Quanto deve valere $\lambda$ affinché $\PP(U+V>3)>1/2$?
\end{enumerate}

\Esercizio{} %16
Siano $X$ e $Y$ due variabili aleatorie congiuntamente gaussiane con medie $\mu_X$ e $\mu_Y$, varianze $\sigma_X^2$ e $\sigma_Y^2$, coefficiente di correlazione $\rho_{X,Y}$. Detta $W=X-Y$, mostrare che $U=|W-\EE[W]|$ è una variabile aleatoria continua, e calcolarne la densità.

\Esercizio{} %17
Sia $(X,Y)$ un vettore aleatorio con funzione caratteristica
\[
\varphi_{(X,Y)}(u,v)=\exp\left\{i(u+2v)-\frac{1}{2}\left(3u^2+10uv+9v^2\right)  \right\}
\]
\begin{enumerate}
\item [(a)] Riconoscere la legge di $(X,Y)$ e scrivere il vettore delle medie e la matrice varianza.
\item [(b)] Siano $U=\lambda X$ e $V=X-\lambda Y$, con $\lambda\in\RR$. Determinare la legge del vettore aleatorio $(U,V)$.
\item [(c)] Trovare l'unico valore positivo di $\lambda$ tale per cui le variabili aleatorie $U$ e $V$ siano indipendenti.
\end{enumerate}

\Esercizio{} %18
Si consideri il vettore aleatorio $(X_\alpha,Y_\alpha)\sim\NNN\left(\begin{pmatrix}
0 \\1
\end{pmatrix},\begin{pmatrix}
2\alpha &1 \\1&1
\end{pmatrix}\right)$.
\begin{enumerate}
\item [(a)] Determinare i valori ammissibili del parametro $\alpha\in\RR$.
\item [(b)] Riconoscere le distribuzioni di $X_\alpha$ e $Y_\alpha$.
\item [(c)] Determinare il valore del parametro $\overline{\alpha}\in\RR$ tale che $\EE[(X_{\overline{\alpha}}-Y_{\overline{\alpha}})^2]=2$.
\item [(d)] Si consideri ora il vettore $(X_{\overline{\alpha}},Y_{\overline{\alpha}})$. Calcolare $\PP(\max\{X_{\overline{\alpha}},  Y_{\overline{\alpha}}\}=Y_{\overline{\alpha}})$. 
\end{enumerate}

\Esercizio{} %19
Siano $X$ e $Z$ due variabili aleatorie indipendenti tali che $X\sim\NNN(0,1)$ e $Z$ abbia legge: $\PP(Z=1)=1/2$ e $\PP(Z=-1)=1/2$. Si ponga $Y=ZX$.
\begin{enumerate}
\item [(a)] Che legge ha $Y$?
\item [(b)] Calcolare $Cov(X,Y)$.
\item [(c)] Mostrare che $X+Y$ non è una variabile aleatoria gaussiana.
\item [(d)] $(X,Y)$ è un vettore gaussiano?
\item [(e)] $X$ e $Y$ sono indipendenti?
\end{enumerate}

\Esercizio{} %20
Si provi che se $X_1,\dots,X_n$ sono variabili aleatorie i.i.d. $\NNN(\mu,\sigma^2)$, con $\sigma^2>0$, allora 
\[
\sum_{k=1}^{n}\frac{(X_k-\mu)^2}{\sigma^2}\sim\chi^2(n)=\Gamma\left(\frac{n}{2},\frac{1}{2}  \right).
\]

\Esercizio{} %21
Sia $X=(X_1,\dots,X_n)\sim\NNN(\mu,C)$ un vettore aleatorio gaussiano con $C$ invertibile. Si mostri che la variabile aleatoria $(X-\mu)^TC^{-1}(X-\mu)$ ha legge $\chi^2(n)$.

\Esercizio{} %22
Siano $X_1,\dots,X_n$ variabili aleatorie reali i.i.d. $\NNN(\mu,\sigma^2)$.  Si considerino
\[
\overline{X}_n=\frac{1}{n}\sum_{k=1}^nX_k,\qquad Y_k=X_k-\overline{X},\qquad S_n^2=\frac{1}{n-1}\sum_{k=1}^n(X_k-\overline{X}_n)^2.
\]
\begin{enumerate}
\item [(a)] Si calcoli $\varphi_{(\overline{X}_n,Y_1,\dots,Y_n)}$.
\item [(b)] Se ne deduca l'indipendenza di $\overline{X}_n$ e $S_n^2$.
\item [(c)] Si mostri che $Z=\dfrac{\overline{X}_n-\mu}{\sigma/\sqrt{n}}\sim\NNN(0,1)$.
\item [(d)] Sapendo che $Q=\dfrac{n-1}{\sigma^2}\ S_n^2\sim\chi^2(n-1)$, si calcoli la funzione caratteristica di $\varphi_{(\overline{X}_n,S_n^2)}$.
\item [(e)] Si calcolino le funzioni caratteristiche $\varphi_{(\overline{X}_n^2,S_n^2)}$ e $\varphi_{\overline{X}_n^2+S_n^2}$ nel caso $\mu=0$.
\item [(f$^*$)] Si trovi la densità continua della variabile aleatoria $T=\dfrac{\overline{X}_n-\mu}{\sqrt{S_n^2/n}}$.
\end{enumerate}

\ParteSoluzioni

\Soluzione{}